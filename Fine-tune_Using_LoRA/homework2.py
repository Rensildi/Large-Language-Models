# -*- coding: utf-8 -*-
"""Homework2_Dont_forget_to_save_files.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AvLnCRvZaYmQz1UnyOTlb4Bo33qTkwu4
"""

!pip install datasets pandas pyarrow

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

from datasets import load_dataset
from sklearn.model_selection import train_test_split
import pandas as pd
data = load_dataset("meta-llama/Llama-3.2-1B-Instruct-evals",
        name="Llama-3.2-1B-Instruct-evals__mmlu__details",
        split="latest"
)
# Convert dataset to Pandas DataFrame for easy manipulation
df = data.to_pandas()

# Split into train (80%) and test (20%)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Convert back to Hugging Face dataset format
from datasets import Dataset
train_data = Dataset.from_pandas(train_df)
test_data = Dataset.from_pandas(test_df)

# accuracy by subtask
import seaborn as sns
import matplotlib.pyplot as plt

accuracy_by_subtask = df.groupby("subtask_name")["is_correct"].mean()
print(accuracy_by_subtask)

import seaborn as sns
import matplotlib.pyplot as plt

# Checking performance by task type/subtask
print("Checking performance by task type/subtask:\n")
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
accuracy_by_task = df.groupby("task_name")["is_correct"].mean()
print(accuracy_by_task)
print("\n\n\n")

# Visualization as well
print("Vizualization:\n")
accuracy_by_task.sort_values().plot(kind="barh", figsize=(10, 6))
plt.xlabel("Accuracy")
plt.ylabel("Task")
plt.title("Model Accuracy Across Different Tasks")
plt.show()

print("\n\n\n")

# Analyzing output metrics
print("Analyzing output metrics:\n")
print(df["output_metrics"].head)
df["parsed_accuracy"] = df["output_metrics"].apply(lambda x: x.get("accuracy", None) if isinstance(x, dict) else None)
print(df[["parsed_accuracy"]].dropna().head())  # Show non-null values

print("\n\n\n")

# Extract Accurace from output metrics
print("Extract Accurace from output metrics:\n")
df["parsed_accuracy"] = df["output_metrics"].apply(lambda x: x.get("acc", None) if isinstance(x, dict) else None)
print(df["parsed_accuracy"].head(10))  # Check if values are extracted correctly

print("\n\n\n")

# Compute Overall model accuracy
print("Compute overall model accuracy:\n")
overall_accuracy = df["parsed_accuracy"].mean()
print(f"Overall Model Accuracy: {overall_accuracy:.2%}")

print("\n\n\n")

# Visualization of overall model accuracy
accuracy_by_task.plot(kind="bar", figsize=(12, 6))
plt.xlabel("Task")
plt.ylabel("Accuracy")
plt.title("Model Accuracy Across Different Tasks")
plt.xticks(rotation=90)
plt.show()

print("\n\n\n")

# Comparing Model Predictions vs Correct Responses
## Model Mistakes
print("Model Mistakes:\n")
incorrect_answers = df[df["is_correct"] == False][["input_question", "input_correct_responses", "output_prediction_text"]]
print(incorrect_answers.head(10))
## Most common mistakes
print("Most common mistakes:\n")
df[df["is_correct"] == False]["input_question"].value_counts().head(10)

print("\n\n\n")

#Finding the most difficult subtasks
print("Finding the most difficult subtasks:\n")
hardest_subtasks = accuracy_by_subtask.nsmallest(5)
print(hardest_subtasks)

#Visualizing
print("Vizualizing:\n")
hardest_subtasks.plot(kind="bar", figsize=(8,5), color="red")
plt.ylabel("Accuracy")
plt.title("Worst Performing Subtasks")
plt.show()

print("\n\n\n")

#Find Bias in Performance
print("Find Bias in Performance:\n")
df.boxplot(column="is_correct", by="task_type", vert=False, figsize=(10, 5))
plt.title("Accuracy Distribution by Task Type")
plt.show()

print("\n\n\n")

# Finding the hardest and easiest tasks
print("Finding the hardest and easiest tasks:\n")
# Hardest Tasks (Lowest Accuracy)
hardest_tasks = accuracy_by_task.nsmallest(5)
print("Hardest Tasks:")
print(hardest_tasks)
print("\n")
# Easiest Tasks (Highest Accuracy)
easiest_tasks = accuracy_by_task.nlargest(5)
print("Easiest Tasks:")
print(easiest_tasks)

print("\n\n\n")

#Checking if the model struggles with certain answer choices
print("Checking if the model struggles with certain answer choices:\n")
print(df["output_choice_negative_log_likelihoods"].head())

print("\n\n\n")

# Invastigating Formatting Errors
print("Invastigating formatting errors:\n")
df["correct_format"] = df["output_metrics"].apply(lambda x: x.get("correct_format", None) if isinstance(x, dict) else None)
formatting_issues = df[df["correct_format"] == 0]
print(f"Number of Formatting Issues: {len(formatting_issues)}")
# Identifying Incorrect Answers
print("Identifying ancorrect answers:\n")
incorrect_answers = df[df["parsed_accuracy"] == 0][["input_question", "input_correct_responses", "output_prediction_text"]]
print(incorrect_answers.head(10))  # Show 10 incorrect cases

!pip install transformers peft bitsandbytes accelerate datasets

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
from datasets import load_dataset
from google.colab import userdata

HF_TOKEN = userdata.get('HF_TOKEN')

# Load dataset from Hugging Face
dataset = load_dataset("meta-llama/Llama-3.2-1B-Instruct-evals",
                       name="Llama-3.2-1B-Instruct-evals__mmlu__details",
                       split="latest")

# Split dataset into train and test sets
dataset = dataset.train_test_split(test_size=0.1)
train_data = dataset["train"]
test_data = dataset["test"]

device = "cuda" if torch.cuda.is_available() else "cpu"
# Load LLaMA model and tokenizer
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,
                                             device_map="auto",
                                             torch_dtype=torch.float16,
                                             token=HF_TOKEN)

# Set up LoRA configuration
lora_config = LoraConfig(
    r=1,                # LoRA rank
    lora_alpha=32,      # Scaling factor
    lora_dropout=0.1,   # Dropout to prevent overfitting
    bias="none",        # No bias updates
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Ensure padding token is set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token for padding

# Function to format dataset into Q&A style
def format_example(example):
    question = example["input_question"]
    answer = example["output_parsed_answer"]  # Use the parsed answer
    if answer is None:
        answer = ""  # Avoid training on NaN values
    formatted_text = f"Q: {question}\nA: {answer}"
    return {"text": formatted_text}

# Apply formatting
train_data = train_data.map(format_example)
test_data = test_data.map(format_example)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding=True, truncation=True, max_length=128)

# Tokenize datasets
tokenized_train = train_data.map(tokenize_function, batched=True)
tokenized_test = test_data.map(tokenize_function, batched=True)

# Custom collate function to add labels (shifted input_ids)
def collate_fn(batch):
    input_ids = [example['input_ids'] for example in batch]
    attention_mask = [example['attention_mask'] for example in batch]

    # Shift input_ids to create labels (shifted by 1 for causal LM)
    labels = [example['input_ids'][1:] + [tokenizer.pad_token_id] for example in batch]
    labels = torch.tensor(labels)

    # Ensure the tensors are padded correctly
    return {
        'input_ids': torch.tensor(input_ids),
        'attention_mask': torch.tensor(attention_mask),
        'labels': labels  # Pass the labels for loss calculation
    }

# Custom Trainer with time-based logging
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.last_log_time = time.time()  # Initialize last log time

    def log(self, logs, start_time=None):  # Accept start_time
        # Check if 5-10 seconds have passed since the last log
        current_time = time.time()
        time_elapsed = current_time - self.last_log_time

        if time_elapsed >= 5:  # Log every 5 seconds
            super().log(logs, start_time)  # Pass the start_time along
            self.last_log_time = current_time  # Update last log time
            print(f"Step: {self.state.global_step}, Loss: {logs.get('loss', 'N/A')}")

    def on_epoch_end(self):
        print(f"Epoch {self.state.epoch} finished!")
        super().on_epoch_end()


# Define training arguments with updated parameter names
training_args = TrainingArguments(
    output_dir="./lora-finetuned",
    eval_strategy="epoch",  # Updated from `evaluation_strategy`
    save_strategy="epoch",
    num_train_epochs=10,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=1,  # Log every step for debugging (this is now overridden by time-based logging)
    save_total_limit=2,
    push_to_hub=False,
    report_to="none",  # Disable external logging services
)

# Create the trainer instance
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    data_collator=collate_fn  # Pass custom collator function
)

# Start training
trainer.train()

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained("./lora-finetuned/checkpoint-31600", device_map="auto")

print("Model and tokenizer loaded successfully!")

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
model = AutoModelForCausalLM.from_pretrained("./lora-finetuned/checkpoint-31600", device_map={"": "cuda"})

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

# Adjusted input text to make the question clearer
input_text = "Q: Blood clots are responsible for repeatedly blocking a catheter. What should you do?"

# Tokenize input text and include attention mask
input_data = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

# Ensure the input is moved to the correct device
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate the response with updated parameters for better output
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=100,  # Keep response length reasonable
    num_return_sequences=1,  # Generate one sequence
    temperature=0.3,  # Slightly increased randomness
    top_p=0.9,  # Nucleus sampling to increase diversity
    do_sample=True  # Allow sampling instead of greedy search
)

# Decode the generated output
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Generated Answer:", generated_text)

from google.colab import drive
drive.mount('/content/drive')

"""1. Added the evaluations after fine-tuning.
2. No changes in performance
3. Added prompt questions
  - Responses are varying everytime is being run.
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")

# Compute Accuracy by Task
accuracy_by_task = df.groupby("task_name")["is_correct"].mean()
print("\nAccuracy by Task:")
print(accuracy_by_task)

# Plot Accuracy by Task
plt.figure(figsize=(12, 6))
accuracy_by_task.sort_values().plot(kind="bar", color="skyblue")
plt.xlabel("Task")
plt.ylabel("Accuracy")
plt.title("Model Accuracy Across Different Tasks")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# Extract Accuracy from Output Metrics
df["parsed_accuracy"] = df["output_metrics"].apply(lambda x: x.get("accuracy", None) if isinstance(x, dict) else None)

# Compute Overall Model Accuracy from Output Metrics
overall_model_accuracy = df["parsed_accuracy"].mean()
print(f"\nOverall Model Accuracy (from output metrics): {overall_model_accuracy:.2%}")

# Identify Most Difficult Subtasks
if "subtask_name" in df.columns:  # Ensure subtasks exist
    accuracy_by_subtask = df.groupby("subtask_name")["is_correct"].mean()
    hardest_subtasks = accuracy_by_subtask.nsmallest(5)
    print("\nHardest Subtasks:")
    print(hardest_subtasks)

    # Plot Hardest Subtasks
    hardest_subtasks.plot(kind="bar", figsize=(8, 5), color="red")
    plt.ylabel("Accuracy")
    plt.title("Worst Performing Subtasks")
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.show()

# Model Mistakes - Incorrect Predictions
incorrect_answers = df[df["is_correct"] == False][["input_question", "input_correct_responses", "output_prediction_text"]]
print("\nModel Mistakes (Incorrect Predictions):")
print(incorrect_answers.head(10))

# Identifying Incorrect Answers
print("\nIdentifying Incorrect Answers:")
incorrect_predictions = df[df["parsed_accuracy"] == 0][["input_question", "input_correct_responses", "output_prediction_text"]]
print(incorrect_predictions.head(10))

# Finding Bias in Performance
plt.figure(figsize=(10, 5))
df.boxplot(column="is_correct", by="task_name", vert=False)
plt.title("Accuracy Distribution by Task Type")
plt.xlabel("Task Type")
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()
################################################
# Prompting:
# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# List of structured questions with better formatting
questions = [
    "Q: What is the capital of France?\nAnswer:",
    "Q: What is 5 + 3?\nAnswer:",
    "Q: Who wrote 'Hamlet'?\nAnswer:",
    "Q: When did World War II end?\nAnswer:",
    "Q: What is the largest planet in our solar system?\nAnswer:"
]

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,  # Keep response length reasonable
    num_return_sequences=1,  # Generate one sequence per question
    temperature=0.7,  # Increase randomness slightly
    top_p=0.9,  # Increase diversity in sampling
    do_sample=True  # Allow sampling instead of greedy search
)

# Decode and print results for each question
generated_answers = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")



print("\nEvaluation Completed Successfully!")

"""1. Batch Processing multiple questions.
2. Properly moving data to GPU.
3. Using batach_decode to process all responses at once.
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:
# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# List of structured questions with better formatting
questions = [
    "Q: What is the capital of France?\nAnswer:",
    "Q: What is 5 + 3?\nAnswer:",
    "Q: Who wrote 'Hamlet'?\nAnswer:",
    "Q: When did World War II end?\nAnswer:",
    "Q: What is the largest planet in our solar system?\nAnswer:"
]

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,  # Keep response length reasonable
    num_return_sequences=1,  # Generate one sequence per question
    temperature=0.7,  # Increase randomness slightly
    top_p=0.9,  # Increase diversity in sampling
    do_sample=True  # Allow sampling instead of greedy search
)

# Decode and print results for each question
generated_answers = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""1. Proper Prompt Formatting "Answer:" instead of "A:"
2. set pad_token
3. Tweaked Generation Settings.
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:
# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# List of structured questions with better formatting
questions = [
    "Q: What is the capital of France?\nAnswer:",
    "Q: What is 5 + 3?\nAnswer:",
    "Q: Who wrote 'Hamlet'?\nAnswer:",
    "Q: When did World War II end?\nAnswer:",
    "Q: What is the largest planet in our solar system?\nAnswer:"
]

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,  # Keep response length reasonable
    num_return_sequences=1,  # Generate one sequence per question
    temperature=0.7,  # Increase randomness slightly
    top_p=0.9,  # Increase diversity in sampling
    do_sample=True  # Allow sampling instead of greedy search
)

# Decode and print results for each question
generated_answers = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""1. Modified Prompt Formatting
2. Adding tokenizer.pad_token = tokenizer.eos_token before tokenizing input.
3. Adjusting Generation Parameters
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:

# List of structured questions with better formatting
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,
    num_return_sequences=1,
    temperature=0.7,
    top_p=0.9,
    top_k=50,  # Filters unlikely words
    repetition_penalty=1.2,  # Reduces repetition
    do_sample=True,
    eos_token_id=model.config.eos_token_id  # Ensures stopping at EOS
)


# Decode and print results for each question
generated_answers = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""Fixing generated parameters."""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:

# List of structured questions with better formatting
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,
    num_return_sequences=1,
    temperature=0.6,  # Lower randomness
    top_p=0.9,
    top_k=40,  # Prevents nonsensical words
    repetition_penalty=1.5,  # Avoids repeating input text
    do_sample=True,
    eos_token_id=model.config.eos_token_id  # Ensures stopping
)

# Decode output
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""Increasing repetition penalty"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:

# List of structured questions with better formatting
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,
    num_return_sequences=1,
    temperature=0.6,  # Lower randomness
    top_p=0.9,
    top_k=40,  # Prevents nonsensical words
    repetition_penalty=2.0,  # Avoids repeating input text
    do_sample=True,
    eos_token_id=model.config.eos_token_id  # Ensures stopping
)

# Decode output
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""1. Changed the temperature to 0.4
2. Changed the repetition_penalty to 2.0
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Prompting:

# List of structured questions with better formatting
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Ensure the PAD token is set correctly
tokenizer.pad_token = tokenizer.eos_token

# Tokenize all questions at once (batch processing)
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate responses for all questions
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,
    num_return_sequences=1,
    temperature=0.4,  # Lower randomness
    top_p=0.9,
    top_k=40,  # Prevents nonsensical words
    repetition_penalty=2.0,  # Avoids repeating input text
    do_sample=True,
    eos_token_id=model.config.eos_token_id  # Ensures stopping
)

# Decode output
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Display all results
for question, answer in zip(questions, generated_answers):
    print(f"{question} {answer}")

"""1. Repetition penalty changed to 1.5 to discourage repeating the same phrases.
2. No Repeat N-Gram Size set to 2 to avoid repeating words/phrases.
3. Lowered the temperature
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Ensure pad_token is set to eos_token
tokenizer.pad_token = tokenizer.eos_token

# Example input questions
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Tokenize input
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move input tensors to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate response with updated parameters
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=50,
    num_return_sequences=1,
    temperature=0.4,  # Reduced randomness
    top_p=0.9,
    top_k=40,  # Limits to more probable tokens
    repetition_penalty=1.5,  # Avoids repeating input
    no_repeat_ngram_size=2,  # Prevents repeated phrases/words
    eos_token_id=tokenizer.eos_token_id  # Proper stopping
)

# Decode and print answers
for i, output in enumerate(output_ids):
    answer = tokenizer.decode(output, skip_special_tokens=True)
    print(f"Generated Answer {i+1}:\n{answer}\n")

"""1. Increased max_length to 100
2. Adjusted temperature top_p and top_k
3. Repetition Penalty and No Repat N-Grams changed
"""

import torch
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

import os
# Free up GPU memory
torch.cuda.empty_cache()

# Avoid memory fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load the tokenizer and model from the base model (before LoRA)
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Manually set device_map to avoid offloading
# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit quantization (more memory-efficient than 8-bit)
    bnb_4bit_compute_dtype="float16",  # Use float16 for reduced memory
    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offload for unsupported layers
)

# Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "./lora-finetuned/checkpoint-31600",
    quantization_config=quantization_config,
    device_map="auto"  # Auto-dispatch model to available GPU memory
)

print("Model loaded successfully with 4-bit quantization!")

print("Model and tokenizer loaded successfully!")

# Ensure pad token is set
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token
model.eval()  # Set the model to evaluation mode

print("Model and tokenizer loaded successfully!\n")

# Ensure DataFrame `df` has the required columns
required_columns = [
    "is_correct", "task_name", "output_metrics",
    "input_question", "input_correct_responses",
    "output_prediction_text", "output_choice_negative_log_likelihoods"
]

if not all(col in df.columns for col in required_columns):
    missing_cols = [col for col in required_columns if col not in df.columns]
    raise ValueError(f"Missing columns in dataset: {missing_cols}")

# Compute Overall Accuracy
accuracy = df["is_correct"].mean()
print(f"Overall Accuracy: {accuracy:.2%}")
################################################
# Ensure pad_token is set to eos_token
tokenizer.pad_token = tokenizer.eos_token

# Input questions
questions = [
    "Question: What is the capital of France?\nAnswer:",
    "Question: What is 5 + 3?\nAnswer:",
    "Question: Who wrote 'Hamlet'?\nAnswer:",
    "Question: When did World War II end?\nAnswer:",
    "Question: What is the largest planet in our solar system?\nAnswer:"
]

# Tokenize input text
input_data = tokenizer(questions, return_tensors="pt", padding=True, truncation=True)

# Move input to GPU
input_ids = input_data.input_ids.to("cuda")
attention_mask = input_data.attention_mask.to("cuda")

# Generate response
output_ids = model.generate(
    input_ids,
    attention_mask=attention_mask,
    max_length=100,  # Increased max_length for better response completion
    num_return_sequences=1,
    temperature=0.6,  # Slightly higher temperature for diversity
    top_p=0.9,
    top_k=40,
    repetition_penalty=1.5,  # Prevents repeating phrases
    no_repeat_ngram_size=2,
    eos_token_id=tokenizer.eos_token_id
)

# Decode and print the answers
for i, output in enumerate(output_ids):
    answer = tokenizer.decode(output, skip_special_tokens=True)
    print(f"Generated Answer {i+1}:\n{answer}\n")